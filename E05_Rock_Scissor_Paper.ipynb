{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcbea3e",
   "metadata": {},
   "source": [
    "# E05_Rock_Scissor_Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91cf50",
   "metadata": {},
   "source": [
    "## Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e3b0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b503d",
   "metadata": {},
   "source": [
    "## Load data, Resize Rock, Scissor, paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0b42af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650  images to be resized.\n",
      "650  images resized.\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_/scissor\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b0b898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630  images to be resized.\n",
      "630  images resized.\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_/paper\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24df3fd",
   "metadata": {},
   "source": [
    "## Make train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b7a5840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 1910 입니다.\n",
      "x_train shape: (2000, 28, 28, 3)\n",
      "y_train shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(img_path, number_of_data=2000):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper_\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1dffcbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1e651ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (1173, 28, 28, 3)\n",
      "y_test shape: (1173,)\n"
     ]
    }
   ],
   "source": [
    "# x_test, y_test를 만드는 방법은 x_train, y_train을 만드는 방법과 아주 유사합니다.\n",
    "def load_data(ima_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=1173 # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(ima_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(ima_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(ima_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "test_dir = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_test, y_test)=load_data(test_dir)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85900a2",
   "metadata": {},
   "source": [
    "## Make deep learing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13e2b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_channel_3 = 128\n",
    "n_dense=512\n",
    "\n",
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57fb808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 3)  \n",
    "x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a49d1",
   "metadata": {},
   "source": [
    "## Evaluate model by test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c540eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 160,835\n",
      "Trainable params: 160,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 3s 149ms/step - loss: 1.2587 - accuracy: 0.3715 - val_loss: 1.1207 - val_accuracy: 0.8295\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 1.1494 - accuracy: 0.4585 - val_loss: 0.7575 - val_accuracy: 0.8295\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 0.9447 - accuracy: 0.5690 - val_loss: 0.9201 - val_accuracy: 0.8295\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 3s 147ms/step - loss: 0.7514 - accuracy: 0.6355 - val_loss: 0.9495 - val_accuracy: 0.8295\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 3s 143ms/step - loss: 0.6833 - accuracy: 0.6605 - val_loss: 0.9274 - val_accuracy: 0.8295\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.5774 - accuracy: 0.7510 - val_loss: 1.1248 - val_accuracy: 0.8295\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.4488 - accuracy: 0.8585 - val_loss: 1.1106 - val_accuracy: 0.8295\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 3s 147ms/step - loss: 0.3160 - accuracy: 0.9085 - val_loss: 1.1455 - val_accuracy: 0.8295\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.2148 - accuracy: 0.9480 - val_loss: 1.1771 - val_accuracy: 0.8295\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 0.1817 - accuracy: 0.9565 - val_loss: 1.2021 - val_accuracy: 0.8295\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.1554 - accuracy: 0.9620 - val_loss: 1.1137 - val_accuracy: 0.8295\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 0.1325 - accuracy: 0.9710 - val_loss: 1.2784 - val_accuracy: 0.8295\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.1006 - accuracy: 0.9860 - val_loss: 1.2917 - val_accuracy: 0.8295\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.1001 - accuracy: 0.9840 - val_loss: 1.1548 - val_accuracy: 0.8295\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 3s 147ms/step - loss: 0.0836 - accuracy: 0.9900 - val_loss: 1.2182 - val_accuracy: 0.8295\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.0863 - accuracy: 0.9845 - val_loss: 1.2430 - val_accuracy: 0.8295\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.0714 - accuracy: 0.9905 - val_loss: 1.3779 - val_accuracy: 0.8295\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.0682 - accuracy: 0.9905 - val_loss: 1.2166 - val_accuracy: 0.8295\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.0643 - accuracy: 0.9935 - val_loss: 1.1549 - val_accuracy: 0.8295\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.0622 - accuracy: 0.9950 - val_loss: 1.2376 - val_accuracy: 0.8295\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 3s 147ms/step - loss: 0.0598 - accuracy: 0.9930 - val_loss: 1.2445 - val_accuracy: 0.8295\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 0.0566 - accuracy: 0.9945 - val_loss: 1.3535 - val_accuracy: 0.8295\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 0.0491 - accuracy: 0.9955 - val_loss: 1.0792 - val_accuracy: 0.8295\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.0490 - accuracy: 0.9960 - val_loss: 1.2317 - val_accuracy: 0.8295\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 3s 143ms/step - loss: 0.0431 - accuracy: 0.9980 - val_loss: 1.3083 - val_accuracy: 0.8295\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.0433 - accuracy: 0.9955 - val_loss: 1.2271 - val_accuracy: 0.8295\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 3s 147ms/step - loss: 0.0393 - accuracy: 0.9985 - val_loss: 1.2402 - val_accuracy: 0.8295\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.0405 - accuracy: 0.9965 - val_loss: 1.0782 - val_accuracy: 0.8295\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.0366 - accuracy: 0.9965 - val_loss: 1.0864 - val_accuracy: 0.8295\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 3s 147ms/step - loss: 0.0337 - accuracy: 0.9970 - val_loss: 1.2142 - val_accuracy: 0.8295\n",
      "37/37 - 0s - loss: 1.2142 - accuracy: 0.8295\n",
      "test_loss: 1.2142486572265625 \n",
      "test_accuracy: 0.8294970393180847\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', \n",
    "                              input_shape=(IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "model.add(keras.layers.MaxPooling2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, kernel_regularizer=keras.regularizers.l2(0.001),activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "history = model.fit(x_train_reshaped, y_train, batch_size=100,epochs=30, \n",
    "                    verbose=1, validation_data=(x_test_reshaped, y_test),\n",
    "                    callbacks =[callback])\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202cde12",
   "metadata": {},
   "source": [
    "## 최종적으론 test_loss: 1.2142486572265625 test_accuracy: 0.8294970393180847\n",
    "## test_accuracy가 82.94 % 나왔다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14d868",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f2d4e",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "첫 시도에는 아이펠 폴더에 있는 기존 데이터로 해보았지만, 정확도가 상당히 낮게 나왔고 볼품 없었다. \n",
    "\n",
    "결국 데이터 수가 문제라고 생각을 해서 직접 웹캠으로 총 1910장의 데이터를 수집했다. 최대한 다양한 각도에서 많이 찍은 데이터였다. \n",
    "\n",
    "또한, train, test set에서 총 이미지 수를 제대로 기입하지 않아 index 에러가 났었다.\n",
    "\n",
    "딥러닝 모델 설계에서 여러 시도 끝에, 사진에서 가위, 바위, 보 살펴야 할 특징이 많을 것이라고 생각해서 layer를 추가해보았다. 그리고 서치 후에 Dropout, keras 를 사용해서 과적합을 줄이도록 했다. \n",
    "\n",
    "그 결과 약 83%의 정확도가 나왔다. 데이터는 많을 수록 좋고, 과적합은 적절히 줄여야 좋은 모델이 완성이 되는 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
