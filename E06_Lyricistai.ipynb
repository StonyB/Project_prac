{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cf1d2d",
   "metadata": {},
   "source": [
    "# Lyricist ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e123dd5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1844344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "import re    \n",
    "import glob\n",
    "import numpy as np         \n",
    "import tensorflow as tf    \n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e4b9d",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81af2c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busted flat in Baton Rouge, waitin' for a train\n",
      "And I's feelin' near as faded as my jeans\n",
      "Bobby thumbed a diesel down, just before it rained\n",
      "It rode us all the way to New Orleans I pulled my harpoon out of my dirty red bandanna\n",
      "I was playin' soft while Bobby sang the blues, yeah\n",
      "Windshield wipers slappin' time, I was holdin' Bobby's hand in mine\n",
      "We sang every song that driver knew Freedom's just another word for nothin' left to lose\n",
      "Nothin', don't mean nothin' hon' if it ain't free, no no\n",
      "And, feelin' good was easy, Lord, when he sang the blues\n",
      "You know, feelin' good was good enough for me\n",
      "Good enough for me and my Bobby McGhee From the Kentucky coal mine to the California sun\n",
      "There Bobby shared the secrets of my soul\n",
      "Through all kinds of weather, through everything we done\n",
      "Yeah, Bobby baby kept me from the cold One day up near Salinas, Lord, I let him slip away\n",
      "He's lookin' for that home, and I hope he finds it\n",
      "But, I'd trade all of my tomorrows, for a single yesterday\n",
      "To be holdin' Bobby's body next to mine Freedom's just another word for nothin' left to lose\n",
      "Nothin', that's all that Bobby left me, yeah\n",
      "But, feelin' good was easy, Lord, when he sang the blues\n",
      "Hey, feelin' good was good enough for me, mm-hmm\n",
      "Good enough for me and my Bobby McGhee La da da\n",
      "La da da da\n",
      "La da da da da da da da\n",
      "La da da da da da da da\n",
      "Bobby McGhee, yeah La da da da da da da\n",
      "La da da da da da da\n",
      "La da da da da da da\n",
      "Bobby McGhee, yeah La da La la da da la da da la da da\n",
      "La da da da da da da da da\n",
      "Hey, my Bobby\n",
      "Oh, my Bobby McGhee, yeah La la la la la la la la\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx >30: break   # 일단 문장 20개만 확인해 볼 겁니다.\n",
    "    \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3d0f4",
   "metadata": {},
   "source": [
    "## Delete 특수문자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618c9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()   \n",
    "    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "     # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "     # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    sentence = '<start> ' + sentence + ' <end>'      \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726f5a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> busted flat in baton rouge , waitin for a train <end>',\n",
       " '<start> and i s feelin near as faded as my jeans <end>',\n",
       " '<start> bobby thumbed a diesel down , just before it rained <end>',\n",
       " '<start> it rode us all the way to new orleans i pulled my harpoon out of my dirty red bandanna <end>',\n",
       " '<start> i was playin soft while bobby sang the blues , yeah <end>',\n",
       " '<start> windshield wipers slappin time , i was holdin bobby s hand in mine <end>',\n",
       " '<start> we sang every song that driver knew freedom s just another word for nothin left to lose <end>',\n",
       " '<start> nothin , don t mean nothin hon if it ain t free , no no <end>',\n",
       " '<start> and , feelin good was easy , lord , when he sang the blues <end>',\n",
       " '<start> you know , feelin good was good enough for me <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e4207",
   "metadata": {},
   "source": [
    "## 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f5612fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 3604 1691 ...    0    0    0]\n",
      " [   2    8    5 ...    0    0    0]\n",
      " [   2  804    1 ...    0    0    0]\n",
      " ...\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f36aaa97e80>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\t\n",
    "    total_data_text = list(tensor)\n",
    "    num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    maxlen = int(max_tokens)\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, \n",
    "                                                           padding='post',\n",
    "                                                          maxlen=maxlen)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e9c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d37131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 3604 1691   14    1 3995    4 1100   28    9  681    3    0    0\n",
      "    0    0    0    0    0]\n",
      "[3604 1691   14    1 3995    4 1100   28    9  681    3    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <END>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a7891",
   "metadata": {},
   "source": [
    "## Make train, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d33d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (140599, 19)\n",
      "Target Train:  (140599, 19)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)\n",
    "\n",
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cf2980b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 19), (256, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62655815",
   "metadata": {},
   "source": [
    "## Make AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242cde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c33263a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 19, 7001), dtype=float32, numpy=\n",
       "array([[[-2.35608546e-04, -7.64364231e-05,  1.16892450e-04, ...,\n",
       "          1.90172708e-04, -3.65676620e-04,  9.89767068e-05],\n",
       "        [-1.19571458e-04, -3.14243189e-05,  4.00590943e-04, ...,\n",
       "          4.10022010e-04, -7.87389290e-04,  1.90423583e-04],\n",
       "        [ 8.81598316e-05, -2.61214271e-04,  5.67055074e-04, ...,\n",
       "          2.77251762e-04, -1.19517371e-03,  1.79567833e-05],\n",
       "        ...,\n",
       "        [ 1.67099421e-03, -2.55617383e-03, -1.04821078e-03, ...,\n",
       "         -1.90157164e-03,  2.37548654e-03,  1.07440643e-03],\n",
       "        [ 1.84452103e-03, -2.69731833e-03, -1.10692589e-03, ...,\n",
       "         -1.84870732e-03,  2.46710144e-03,  1.28259964e-03],\n",
       "        [ 1.99557515e-03, -2.82869814e-03, -1.16013386e-03, ...,\n",
       "         -1.79553591e-03,  2.53922655e-03,  1.46862795e-03]],\n",
       "\n",
       "       [[-2.35608546e-04, -7.64364231e-05,  1.16892450e-04, ...,\n",
       "          1.90172708e-04, -3.65676620e-04,  9.89767068e-05],\n",
       "        [-2.48197641e-04,  1.38534320e-04,  1.07901178e-04, ...,\n",
       "          1.09694898e-04, -4.66458558e-04,  1.78496164e-04],\n",
       "        [ 2.76756327e-04,  2.55844352e-04, -7.29713793e-06, ...,\n",
       "          1.06287000e-04, -7.85892364e-04,  9.73064016e-05],\n",
       "        ...,\n",
       "        [-9.47466906e-05,  8.47295232e-05, -3.09953466e-04, ...,\n",
       "          6.72662980e-04, -9.08505754e-05,  2.56390977e-05],\n",
       "        [ 5.49196266e-05,  1.66255792e-04, -1.26625062e-04, ...,\n",
       "          3.63716303e-04,  2.95204838e-04, -2.60809175e-05],\n",
       "        [ 3.91238427e-05,  4.53799767e-05, -1.61100208e-04, ...,\n",
       "         -5.11022336e-05,  7.20444135e-04, -1.67902734e-04]],\n",
       "\n",
       "       [[-2.35608546e-04, -7.64364231e-05,  1.16892450e-04, ...,\n",
       "          1.90172708e-04, -3.65676620e-04,  9.89767068e-05],\n",
       "        [-1.21102239e-04,  2.25704018e-04,  3.09954223e-04, ...,\n",
       "          5.72226534e-04, -3.44423868e-04,  4.44594800e-04],\n",
       "        [-4.07449028e-04,  3.82035942e-04,  2.45922449e-04, ...,\n",
       "          7.21816032e-04,  1.64586036e-05,  3.08144721e-04],\n",
       "        ...,\n",
       "        [ 2.93479185e-04, -1.13856746e-03, -3.80287442e-04, ...,\n",
       "         -8.04325857e-04,  1.11454201e-03, -1.38048199e-03],\n",
       "        [ 5.19582361e-04, -1.34389487e-03, -5.31698170e-04, ...,\n",
       "         -1.06182683e-03,  1.44415069e-03, -1.02602632e-03],\n",
       "        [ 7.78553425e-04, -1.54664717e-03, -6.65880856e-04, ...,\n",
       "         -1.25947769e-03,  1.72293908e-03, -6.56113843e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.35608546e-04, -7.64364231e-05,  1.16892450e-04, ...,\n",
       "          1.90172708e-04, -3.65676620e-04,  9.89767068e-05],\n",
       "        [ 1.67494552e-04, -9.83459759e-05,  5.58957618e-05, ...,\n",
       "          3.47899942e-04, -7.72343948e-04,  7.28192827e-05],\n",
       "        [ 4.39182826e-04,  9.10800445e-05, -2.18862435e-04, ...,\n",
       "          1.62375582e-04, -6.95409486e-04,  4.96418797e-04],\n",
       "        ...,\n",
       "        [ 1.29176758e-03, -2.90520053e-04, -3.72229697e-05, ...,\n",
       "         -2.25511240e-03,  1.96080725e-03, -3.26418871e-04],\n",
       "        [ 1.45225413e-03, -5.94109122e-04, -1.62906144e-04, ...,\n",
       "         -2.39893747e-03,  2.20368709e-03, -9.35893695e-05],\n",
       "        [ 1.62755430e-03, -9.02969507e-04, -2.93403253e-04, ...,\n",
       "         -2.48661311e-03,  2.40415032e-03,  1.61131349e-04]],\n",
       "\n",
       "       [[-2.35608546e-04, -7.64364231e-05,  1.16892450e-04, ...,\n",
       "          1.90172708e-04, -3.65676620e-04,  9.89767068e-05],\n",
       "        [-5.25064359e-04,  3.00813204e-04,  5.82344146e-05, ...,\n",
       "         -4.91781691e-07, -4.97075322e-04,  2.84682319e-04],\n",
       "        [-4.95188229e-04,  9.54167801e-04, -1.30634508e-04, ...,\n",
       "         -1.16244606e-04, -6.84925122e-04,  4.25128819e-04],\n",
       "        ...,\n",
       "        [ 1.00188900e-03, -1.97731308e-03, -3.36897210e-04, ...,\n",
       "         -1.91529619e-03,  1.96289737e-03,  4.79512819e-04],\n",
       "        [ 1.20438205e-03, -2.12797476e-03, -4.71450679e-04, ...,\n",
       "         -1.97416800e-03,  2.12957617e-03,  7.04038539e-04],\n",
       "        [ 1.40750234e-03, -2.27672630e-03, -5.93576638e-04, ...,\n",
       "         -1.99767435e-03,  2.26726849e-03,  9.23954532e-04]],\n",
       "\n",
       "       [[-2.35608546e-04, -7.64364231e-05,  1.16892450e-04, ...,\n",
       "          1.90172708e-04, -3.65676620e-04,  9.89767068e-05],\n",
       "        [ 1.67494552e-04, -9.83459759e-05,  5.58957618e-05, ...,\n",
       "          3.47899942e-04, -7.72343948e-04,  7.28192827e-05],\n",
       "        [ 2.90060561e-04, -3.97810974e-04, -1.58084149e-04, ...,\n",
       "          1.42354460e-04, -5.62438858e-04, -7.65906007e-05],\n",
       "        ...,\n",
       "        [ 9.10029776e-05, -1.49043766e-03, -1.11890875e-03, ...,\n",
       "         -2.03441409e-03,  1.59300852e-03, -1.15416129e-03],\n",
       "        [ 2.86527356e-04, -1.63628638e-03, -1.16128882e-03, ...,\n",
       "         -2.17932090e-03,  1.78731198e-03, -6.96085102e-04],\n",
       "        [ 5.10989688e-04, -1.78759778e-03, -1.20349776e-03, ...,\n",
       "         -2.26554088e-03,  1.94649654e-03, -2.58775835e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de7d8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca69c21",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ead7f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "686/686 [==============================] - 125s 179ms/step - loss: 2.7904\n",
      "Epoch 2/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 2.3833\n",
      "Epoch 3/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 2.2380\n",
      "Epoch 4/30\n",
      "686/686 [==============================] - 127s 185ms/step - loss: 2.1341\n",
      "Epoch 5/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 2.0475\n",
      "Epoch 6/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 1.9690\n",
      "Epoch 7/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 1.8952\n",
      "Epoch 8/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 1.8244\n",
      "Epoch 9/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 1.7570\n",
      "Epoch 10/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 1.6912\n",
      "Epoch 11/30\n",
      "686/686 [==============================] - 129s 187ms/step - loss: 1.6271\n",
      "Epoch 12/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 1.5644\n",
      "Epoch 13/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 1.5031\n",
      "Epoch 14/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 1.4428\n",
      "Epoch 15/30\n",
      "686/686 [==============================] - 129s 187ms/step - loss: 1.3841\n",
      "Epoch 16/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 1.3281\n",
      "Epoch 17/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 1.2739\n",
      "Epoch 18/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 1.2224\n",
      "Epoch 19/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 1.1733\n",
      "Epoch 20/30\n",
      "686/686 [==============================] - 129s 187ms/step - loss: 1.1275\n",
      "Epoch 21/30\n",
      "686/686 [==============================] - 129s 187ms/step - loss: 1.0842\n",
      "Epoch 22/30\n",
      "686/686 [==============================] - 129s 187ms/step - loss: 1.0440\n",
      "Epoch 23/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 1.0062\n",
      "Epoch 24/30\n",
      "686/686 [==============================] - 129s 187ms/step - loss: 0.9715\n",
      "Epoch 25/30\n",
      "686/686 [==============================] - 129s 187ms/step - loss: 0.9395\n",
      "Epoch 26/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 0.9103\n",
      "Epoch 27/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 0.8830\n",
      "Epoch 28/30\n",
      "686/686 [==============================] - 128s 186ms/step - loss: 0.8587\n",
      "Epoch 29/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 0.8362\n",
      "Epoch 30/30\n",
      "686/686 [==============================] - 128s 187ms/step - loss: 0.8158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f362af3d550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7f6607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099/1099 - 14s - loss: 0.7723\n",
      "0.7722645401954651\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val,  dec_val, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71aa850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92378111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love the way you lie <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "763a9f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> if you re not ready for <unk> . woah oh oh ! the road of life is rocky <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> if you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68368ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the only thing that keeps me goin <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72397e7",
   "metadata": {},
   "source": [
    "# 최종적으론 validation loss가 0.7722645401954651로 약 0.77로 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324ba5b",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d1d2e",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "이번 프로젝트에서는 Lyricst AI 를 만들어 보았다.\n",
    "\n",
    "이번에도 역시나 텍스트 데이터 전처리 과정의 중요성에 대해 다시 한번 배웠다. 또한, 학습하는 벡터 수가 많아서인지 파라미터 갯수 또한 많아서인지 학습량이 많이 필요한 것 같다. 그리고 모델을 만들 때는 최대한 사람의 인식 패턴과 비슷하게 만들어야 더 좋은 성능을 내는 것 같습니다.\n",
    "\n",
    "최종 결과로는 validation loss가 약 0.77로 나왔으며, generate_text 함수로 문장 형성을 체크했는데 작곡을 잘 하는 좋은 성능을 보여줬다.\n",
    "뛰어난 성능에 놀랐고, GPT-3에 감탄했다. 개인적으로는 Lyricst을 넘어서 글이 되고 논문을 쓰는 모델을 만들어보고 연구해보고 싶다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
